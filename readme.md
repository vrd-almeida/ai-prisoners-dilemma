# ğŸ¤– Prisoner's Dilemma AI Simulation

A simulation framework for studying the classic **Prisonerâ€™s Dilemma** using rule-based agents and reinforcement learning. This project allows agents to play iterated games, adapt strategies using **Q-learning**, and analyze emerging behavior such as cooperation, betrayal, or equilibrium.

---

# Streamlit Deployment

https://ai-prisoners-dilemma-tzx2rjmtv48mkthtwz4qym.streamlit.app/

---

## ğŸ“Œ What is the Prisonerâ€™s Dilemma?

The **Prisoner's Dilemma** is a famous problem in game theory that illustrates why individuals might not cooperate, even when it is in their best interest to do so.

Each of the two players can either:
- **Confess** (betray the other), or
- **Remain Silent** (cooperate).

The outcomes depend on the combination of choices:

| Player A       | Player B       | Years in Prison A | Years in Prison B |
|----------------|----------------|-------------------|-------------------|
| Silent         | Silent         | 1                 | 1                 |
| Confess        | Silent         | 0                 | 10                |
| Silent         | Confess        | 10                | 0                 |
| Confess        | Confess        | 5                 | 5                 |

In this project, we **invert the reward scale** so that **fewer years = higher reward**. This makes it compatible with reinforcement learning agents that aim to **maximize** reward.

---

## ğŸ§  Goals of This Project

- âœ… Simulate the Prisoner's Dilemma as an iterated game
- âœ… Implement basic agents with static strategies (Tit for Tat, Always Confess, etc.)
- âœ… Implement a **Q-learning agent** that learns the best strategy through trial and error
- âœ… Analyze the evolution of behavior over time
- âœ… Visualize score progression, cooperation rates, and learned Q-values

---

## ğŸ§© How It Works

### ğŸ” Repeated Game
Each match consists of multiple rounds. After each round:
- Agents decide based on the opponentâ€™s last move (if they can)
- Scores are assigned based on a payoff matrix
- The Q-learning agent updates its policy using the Q-learning algorithm

### ğŸ§  Q-learning Logic

Q-values are updated using the formula:

Q(s, a) â† Q(s, a) + Î± Ã— [r + Î³ Ã— max(Q(s', a')) âˆ’ Q(s, a)]

yaml
Copier
Modifier

Where:
- `s` is the current state (opponentâ€™s last move)
- `a` is the chosen action
- `r` is the reward received
- `s'` is the new state (next opponent move)
- `Î±` is the learning rate
- `Î³` is the discount factor

---

## ğŸ® Strategies Implemented

| Strategy        | Description                                             |
|-----------------|---------------------------------------------------------|
| AlwaysCooperate | Always plays "Silent"                                   |
| AlwaysDefect    | Always plays "Confess"                                  |
| TitForTat       | Starts with "Silent", then mimics opponentâ€™s last move  |
| QLearningAgent  | Learns the best action using Q-learning from feedback   |

---

ğŸ‘¨â€ğŸ’» Author
Vinicius Rodrigues De Almeida, PhD.

ğŸ“ Based in Lyon, France ğŸ‡«ğŸ‡·

ğŸ’¡ Interests: optimization, machine learning, AI

ğŸ”— https://www.linkedin.com/in/vinicius-rodrigues-de-almeida-2a575084/

